---
title: "Drift and Divergence Factorizations"
author: "Sue Parkinson"
date: "2022-01-17"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

This analysis will explore using EBMF to detect tree structure in some synthetic data sets by computing either the drift or the divergence factorization. The functions used in this file are from `code/drift_div_factorizations.R`. We also import some libraries for plotting purposes.

```{r source r code, message=FALSE, warning=FALSE}
library(RColorBrewer)
library(scales)
source("code/drift_div_factorizations.R")
```

## The Data
I will be using two types of data sets in this analysis. All are based on the model of Brownian motion on a tree. The first type is data sets where the data is generated continuously along the branches of the tree. I call these data sets continuous trees. There are four continuous tree data sets in `data/ContinuousTrees`. The second type of data set is where the data is generated only around the nodes of the tree, which I call node trees. That is, for each node in the tree, a certain number of samples are drawn from a normal distribution centered at that node. There are nine node tree data sets in `data/NodeTrees`. A third type of data set of interest in some applications is where the data is only generated at the leaves of the tree. These data sets can be formed from node tree data sets by filtering on the column `IsLeaf`.

The function `form_tree_from_file` in `code/drift_div_factorizations.R` parses the raw data into a format that can be fed into the various methods. Below, we load one data set of each type.

```{r load data}
nodetree <- form_tree_from_file('data/NodeTrees/NodeTree4/NodeTree4.csv')
continuoustree <- form_tree_from_file('data/ContinuousTrees/tree1.csv')
```

One way to understand the trees is by examining their covariance matrix. For example, here is an image of the covariance matrix of the first node tree data set, where the spots in the matrix are colored by their value.
```{r cov plot nodetree}
image.plot(cov(t(nodetree$matrix)))
```
You can see that there are nested blocks in the matrix. Another way to understand the trees is by plotting a dimensionality reduction. The data set can be reduced to two dimensions (here we do so using tsne), and then we can draw arrows between parents and their children.
```{r nodetree plot}
plot_nodetree <- function(tree,color=1+tree$labels,palette=c("#0000D2","#D2D2D2","#D20000")){
  #create colormap
  ## create color palette function
  pal <- colorRamp(palette)
  ## rescale color to be between 0 and 1, and 0.5 being the "zero" point
  newcolor <- color/max(abs(color)) / 2 + 0.5
  ## use pal and alpha to get rgb value for color 
  newcolor <- alpha(rgb(pal(newcolor)/255),0.4)
  ## make colored scatter plot
  plot(tree$dimred,col=newcolor,pch=20)
  
  # draw arrows between parents and children
  ## get all the different labels of the nodes
  labels <- unique(tree$labels)
  ## get the nodes that are parents
  parents <- labels[1:(length(labels)/2)]
  ## initialize index of child and num children per node
  childidx <- 2 
  children_per_node <- 2
  ## draw arrows
  for (parent in parents){
    for (childnum in 1:children_per_node){
      child <- labels[childidx]
      arrows(mean(tree$dimred$tsne0[tree$labels == parent]),
             mean(tree$dimred$tsne1[tree$labels == parent]),
             mean(tree$dimred$tsne0[tree$labels == child]),
             mean(tree$dimred$tsne1[tree$labels == child]),
             length=0.05)
      childidx <- childidx + 1
    }
  }
}
plot_nodetree(nodetree,palette=1:20)
```
There are clusters of points around each nodes.

In contrast, here is the covariance matrix and dimensionality reduction of one of the continuous trees plotted with arrows between the start and end of each branch.
```{r cont tree plot}
image.plot(cov(t(continuoustree$matrix)))
plot_continuoustree <- function(tree,color=tree$labels,palette=c("#0000D2","#D2D2D2","#D20000")){
  #create colormap
  ## create color palette function
  pal <- colorRamp(palette)
  ## rescale color to be between 0 and 1, and 0.5 being the "zero" point
  newcolor <- color/max(abs(color)) / 2 + 0.5
  ## use pal and alpha to get rgb value for color 
  newcolor <- alpha(rgb(pal(newcolor)/255),0.4)
  ## make colored scatter plot
  plot(tree$dimred,col=newcolor,pch=20)
  
  # draw arrows along branches
  ## get all the different labels of the nodes
  labels <- unique(tree$labels)
  ## draw arrows
  for (label in labels){
    group <- tree$dimred[tree$labels == label,]
    first <- head(group,1)
    last <- tail(group,1)
    arrows(first$tsne0,first$tsne1,
           last$tsne0,last$tsne1,
           length=0.05)
  }
}
plot_continuoustree(continuoustree,palette=1:7)
```

## The Methods

The functions `drift_fit` and `div_fit` in `code/drift_div_factorizations.R` fit drift and divergence factorizations to the data using EBMF methods. It uses point-Laplace priors for divergence loadings, point-exponential priors for drift loadings, and normal priors on the factors. 

## Performance on Node Trees
We run the methods on node trees.
```{r run nodetree}
nodetree <- run_methods(nodetree,'output/NodeTrees/NodeTree4/EBMFfactors/',Kmax=Inf,eps=2e-2)
```
Here is what the loadings look like for node tree 4.

First, using the drift factorization.
```{r plot loadings nodetree drift, fig.height = 8, fig.width = 6}
loadings = nodetree$trajectory$drift$loadings.pm[[1]]
numloadings = dim(loadings)[2]
par(mfrow = c(ceiling(numloadings/3), 3),mar=c(1,1,1,1))    
for (loadingnum in 1:numloadings){
  plot_nodetree(nodetree,color=loadings[,loadingnum])
}
```
Now, using the divergence factorization.
```{r plot loadings nodetree div, fig.height = 8, fig.width = 6}
loadings = nodetree$trajectory$div$loadings.pm[[1]]
numloadings = dim(loadings)[2]
par(mfrow = c(ceiling(numloadings/3), 3),mar=c(1,1,1,1))    
for (loadingnum in 1:numloadings){
  plot_nodetree(nodetree,color=loadings[,loadingnum])
}
```

## Performance on Continuous Trees
We now run the methods on continuous trees.
```{r run cont tree}
continuoustree <- run_methods(continuoustree,'output/ContinuousTrees/tree1/EBMFfactors/',Kmax=6,eps=1e-2)
```
Here are the results using the drift factorization.
```{r plot loadings cont tree drift, fig.height = 4, fig.width = 6}
loadings = continuoustree$trajectory$drift$loadings.pm[[1]]
numloadings = dim(loadings)[2]
par(mfrow = c(ceiling(numloadings/3), 3),mar=c(1,1,1,1))    
for (loadingnum in 1:numloadings){
  plot_continuoustree(continuoustree,color=loadings[,loadingnum])
}
```
Finally, using the divergence factorization, the loadings can be visualized using this plot.
```{r plot loadings cont tree div, fig.height = 4, fig.width = 6}
loadings = continuoustree$trajectory$div$loadings.pm[[1]]
numloadings = dim(loadings)[2]
par(mfrow = c(ceiling(numloadings/3), 3),mar=c(1,1,1,1))    
for (loadingnum in 1:numloadings){
  plot_continuoustree(continuoustree,color=loadings[,loadingnum])
}
```

## Issues and questions to think about for drift factorization fits

* What should you choose eps to be? 
    * It probably makes sense to use the local false sign rate to decide if a loading should be nonzero.
* Stopping point-- it doesn't stop adding factors when it shouldn't, unfortunately. It does with node data, but not trajectory data
    * maybe some sort of spatial way of determining stopping? Like is this divergence you drawing a line in the sand, or is the split point close enough to the ancestral point that it's a reasonable split
* What happens with non-tree-shaped data?
    * If you put in non-tree-shaped data, you shouldn't detect structure that doesn't exist.
* This factorization doesn't quite give the true drift or divergence factorization because it gives continuous values instead of binary ones. Points that are more "specialized" have larger loadings than those that are less specialized. This doesn't quite match up with our definition of the divergence factorization.
* This works well with node datasets up to about 3 levels of splits. It breaks down a bit for 4 levels. This is likely due to the signal/noise ratio
* When and how should you backfit? Or should we stick with a simple greedy approach?
* The drift factorization seems more reliable in general than the divergence factorization in practice, but I'm not sure why.


